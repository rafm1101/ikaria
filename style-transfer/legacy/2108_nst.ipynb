{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2108-nst.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafm1101/deeplear/blob/main/2108_nst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhDcx0I_3VQ2"
      },
      "source": [
        "# Neural style transfer -- an art of art transferation\n",
        "Transferring the style from one picture to another picture or photograph, that is the idea behind this application of neural networks, and this notebook is my personal way of exploring this technique. It is far more than applying some impressionism filter to an image, this technique extracts stylistic features from one (style) image and applies them to another (content) image. this technique builds upon the way a convolutional neural network reads pictures for a classification task. Main feature is that instead of training a neural network to do this job, a pre-trained network is used without training of any layer. Instead, only the content image is the object that is trained on.\n",
        "\n",
        "Inspiration and curiosity is the related exercise of the Deeplearning Specialization of *Deeplearning.ai* in *coursera*, and to mention is the [original paper](https://arxiv.org/abs/1508.06576) (Gatys et. al) as well as the [tutorial](https://www.tensorflow.org/tutorials/generative/style_transfer?hl=en) at *tensorflow.org*. The latter mentions a newer approach to the style transfer problem, but this for later exploration.\n",
        "\n",
        "Personally, I want to understand the idea, experiment on the code, and get a better understanding of coding. The notebook concentrates on three main parts: Exploring a layer's response, the Gatys et. al algorithm for transfering style, and reconstructing style images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVtuWD6o3VQ6"
      },
      "source": [
        "What is the idea behind that Gatys et al algorithm? Passing an image to a cnn trained on image classification, the image is processed through a sequence of convolutional and pooling layers, each of which producing a representation of the image. Along this sequence, representations contain to an increasing extend information about the content starting from more textual information. The style of a picture is extracted from the filter responses of the different layers. More precisely, it is given by the spatial correlations.\n",
        "The key of understanding the magic of transfering a style of one picture to some input image is that the representation of content and style are to some extend separable, and therefore a manipulation of the style can be performed without changing the content representation: After reconstruction, the global properties, the composition of objects, is preserved, while local features are modified. This is coded in the [second main part](#4).\n",
        "\n",
        "In the [first main part](#2), the feature representations are extracted from the filters of chosen hidden layers and shown as images. The [third and final main part](#5) contains reconstructions of the low level feature representations, they were part of the original paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfiaaGF_3VQ7"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "##1 Preparations\n",
        "###1.1 Packages\n",
        "Import the packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GITTz8RK3VQ9"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.ops import EagerTensor\n",
        "import pprint\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKx_GrfbR-F-"
      },
      "source": [
        "Working on Colab, data are loaded and saven from Ggl Drive, so mount Ggl drive to get access to the pretrained model and further images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX-2YEhXR8e-"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = 'drive/MyDrive/2108-transferstyle/'\n",
        "os.chdir( path )\n",
        "print( os.getcwd() )\n",
        "print( os.listdir() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GHgQyVf3VQ-"
      },
      "source": [
        "###1.2 Load pre-trained nn\n",
        "The model used in the original paper is loaded here. Image size is defined as well as the model is set untrainable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiKE2nic3VQ_"
      },
      "source": [
        "# in case on non-random randomness desired set the seed manually\n",
        "#tf.random.set_seed( 272 ) \n",
        "\n",
        "# image dimensions\n",
        "IMG_WIDTH = 600\n",
        "IMG_HEIGHT = 400\n",
        "\n",
        "pp = pprint.PrettyPrinter( indent=4 )\n",
        "\n",
        "# load pre-trained vgg19 without the dense layers on top\n",
        "# change weights argument to 'imagenet' to download weigths\n",
        "vgg = tf.keras.applications.VGG19(\n",
        "        include_top=False, input_shape=( IMG_HEIGHT, IMG_WIDTH, 3),\n",
        "        weights='vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "\n",
        "vgg.trainable = False\n",
        "pp.pprint(vgg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em2WRUAGQrsX"
      },
      "source": [
        "Retrieve the model's architecture details. The names of the layers are needed as reference later to be able to extract the stylistic features of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Ucajx43VRB"
      },
      "source": [
        "#for layer in vgg.layers:\n",
        "#    print( layer.name )\n",
        "vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsM0CsT_lTjH"
      },
      "source": [
        "###1.3 Load images\n",
        "Images are loaded and preprocessed: Their shape needs to agree with the set up model, and they are converted into tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSaET7ZNg54V"
      },
      "source": [
        "def preprocess_image( file_name, height, width ):\n",
        "  \"\"\" \n",
        "  preprocess images: here loading and scaling only\n",
        "  Arguments:\n",
        "    file_name - including path\n",
        "    height    - height after scaling\n",
        "    width     - width after scaling\n",
        "  Returns:\n",
        "    image -- a tf constant of shape (1, width, height, 3)\n",
        "  \"\"\"\n",
        "  image = np.array( Image.open( file_name ).resize( ( width, height ) ) )\n",
        "  image = tf.constant( np.reshape( image, ( (1,) + image.shape ) ) )\n",
        "\n",
        "  return image\n",
        "\n",
        "def plot_row( images, titles, num=3, n=3, J=np.zeros((1,0)) ):\n",
        "  \"\"\" \n",
        "  plot images next to each other with titles\n",
        "  Arguments:\n",
        "    images - tuple of images or tensor of images\n",
        "    titles - tuple of titles\n",
        "    num - number of images to display\n",
        "    n - max number of in a row\n",
        "    J - array of values of a function to display together with the images\n",
        "        (typically the loss function at the end of the loops through the epochs)\n",
        "  \"\"\"\n",
        "  fig = plt.figure( figsize=(16, 4) )\n",
        "  #print( images.shape )\n",
        "  for i in range( np.minimum( num, images.shape[0] ) ):\n",
        "    ax = fig.add_subplot( 1, n, i+1 )\n",
        "    if not tf.is_tensor( images ):\n",
        "      imshow( images[i] )\n",
        "    else:\n",
        "      imshow( images[i].numpy() )\n",
        "    ax.title.set_text( titles[i] )\n",
        "  \n",
        "  if J.size:\n",
        "    if num == 3:\n",
        "      plot.show()\n",
        "      fig = plt.figure( figsize=(16, 4) )\n",
        "    ax = fig.add_subplot( 1, n, num % 3 + 1 )\n",
        "    plt.plot(J)\n",
        "    ax.title.set_text( 'style loss' )\n",
        "\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81FzfVflpB8C"
      },
      "source": [
        "# images located in the ggl drive directory\n",
        "#content_image_name = 'SAM_0016_math.jpg'\n",
        "content_image_name = 'SAM_0868.JPG'\n",
        "#content_image_name = 'SAM_7044kl.jpg'\n",
        "#style_image_name = 'monet-mohn.jpg'\n",
        "style_image_name = 'van-gogh-noche-estrellada.jpg'\n",
        "\n",
        "# preprocess images\n",
        "content_image = preprocess_image( content_image_name, IMG_HEIGHT, IMG_WIDTH )\n",
        "style_image = preprocess_image( style_image_name, IMG_HEIGHT, IMG_WIDTH )\n",
        "print( type( content_image ) )\n",
        "\n",
        "print(f\"shapes of images:\\n  content - {content_image.shape}\\n    style - {style_image.shape}\\n\")\n",
        "print( content_image.shape )\n",
        "image_list = np.concatenate( [ content_image, style_image ], axis=0 )\n",
        "print( type(image_list) )\n",
        "plot_row( image_list, \n",
        "              [ 'Content_image', 'Style_image' ], 2 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4nrpc3uP8QC"
      },
      "source": [
        "<a name=\"2\"></a>\n",
        "##2 Exploring a layer's response to an image\n",
        "This first main part's objective is to visualise the activations of different hidden layers of an image. These feature maps are 4d tensors of shape (1, *, *, c), where the very first dimension is meaningless for processing a batch of size one only, and the remainder is a quadratic image with c channels, i.e. as many channels as filters used to process the input signal. They are shown as c gray-scale images arranged as a panorama image.\n",
        "\n",
        "First, define helper functions which extract any named layer from the pre-trained model. These layers are used to access the hidden layer's activations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdXPHvmOZD0Y"
      },
      "source": [
        "def get_layer_outputs( model, layer_names ):\n",
        "  \"\"\" little helper function\n",
        "  retrieve list of layer outputs \n",
        "\n",
        "  Arguments:\n",
        "    model -- a tensorflow model, supposed to be the pre-trained one\n",
        "    layer_names -- a list of layer names, supposed to contained in the model\n",
        "\n",
        "  Returns:\n",
        "    layer_outputs -- list of tensors\n",
        "  \"\"\"\n",
        "  if len( layer_names[0] ) == 2:\n",
        "    layer_names = [ x for (x,_) in layer_names ]\n",
        "  layer_outputs = [ layer.output for layer in model.layers if layer.name in layer_names ] \n",
        "  assert len( layer_names ) == len( layer_outputs ), \"Did not find all layers in the model. Compare layer names and model's summary.\"\n",
        "\n",
        "  return layer_outputs\n",
        "\n",
        "def create_layer_output_model( model, outputs ):\n",
        "  \"\"\" little helper function\n",
        "  creates model that returns a list of intermediate output values. (not used)\n",
        "    \n",
        "  Arguments:\n",
        "    model -- a tensorflow model, supposed to be the pre-trained one\n",
        "    outputs -- a list of tensors representing the desired hidden layers to watch\n",
        "  Returns:\n",
        "     model with the desired outputs\n",
        "  \"\"\"\n",
        "    \n",
        "  return tf.keras.Model( inputs = [ model.input ], outputs = outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJvOZ8LWn8TW"
      },
      "source": [
        "Choose the layers by their names and define a helper model that outputs exactly the chosen layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7QyDvyqSwwR"
      },
      "source": [
        "# choose the öayers of interest (check the summary of the model)\n",
        "VISUALIZED_LAYERS = [ 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1' ]\n",
        "#VISUALIZED_LAYERS = [ 'block1_conv2', 'block2_conv2', 'block3_conv4', 'block4_conv4', 'block5_conv4' ]\n",
        "#VISUALIZED_LAYERS = [ 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2' ]\n",
        "\n",
        "# create list of the outputs of these layers\n",
        "layer_outputs = get_layer_outputs( vgg, VISUALIZED_LAYERS )\n",
        "pp.pprint( layer_outputs )\n",
        "\n",
        "# create model\n",
        "vgg_vis = tf.keras.Model( inputs = vgg.input, outputs = layer_outputs) #create_layer_output_model( vgg.input, layer_outputs )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVIWRd3hqCvu"
      },
      "source": [
        "The helper function for displaying the feature maps, which are assumed to be computed already. beserve that the single feature maps are standardised."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvKESdj6afLp"
      },
      "source": [
        "def show_layer_representations( feature_maps, layer_names ):\n",
        "  \"\"\"\n",
        "  Display the feature maps\n",
        "\n",
        "  Arguments:\n",
        "    feature_maps -- predictions of the model's hidden layers\n",
        "    layer_names  -- names of the layers (list of str)\n",
        "  \"\"\"\n",
        "  for layer_name, feature_map in zip( layer_names, feature_maps ):\n",
        "    \n",
        "    # show features of convolution and pooling layers only\n",
        "    if len( feature_map.shape ) == 4:\n",
        "      # shape of feature map is ( 1, h, w, n )\n",
        "      n_C = feature_map.shape[-1]\n",
        "      n_H, n_W = feature_map.shape[ 1:3 ]\n",
        "      # display features in matrix\n",
        "      display_grid = np.zeros( ( n_H, n_W * n_C) )\n",
        "      # fill in processed features\n",
        "      for i in range( n_C ):\n",
        "        display_grid[ :, i * n_W : (i + 1) * n_W ] = feature_map_standardization( feature_map[0, :, :, i] )\n",
        "      # Display the grid\n",
        "      scale = 200. / n_C\n",
        "      plt.figure( figsize=( scale * n_C, scale ) )\n",
        "      plt.title( layer_name )\n",
        "      #plt.grid( False )\n",
        "      plt.imshow(display_grid, aspect='auto', cmap='magma')\n",
        "  \n",
        "def feature_map_standardization( slice ):\n",
        "  \"\"\" little helper's helper function\n",
        "  Standardize and scale a feature map\n",
        "\n",
        "  Arguments:\n",
        "    slice -- slice of a feature map,\n",
        "              np.ndarray ( h, w )\n",
        "  \n",
        "  Returns:\n",
        "    clipped np.ndarray ( h, w ) dtype uint8\n",
        "  \"\"\"\n",
        "  slice -= slice.mean()\n",
        "  s = slice.std()\n",
        "  if s>0: # prevent from dividing by 0, in case it is constant anyways\n",
        "    slice /= slice.std()\n",
        "  slice *= 64\n",
        "  slice += 128\n",
        "  return np.clip( slice, 0, 255 ).astype( 'uint8' )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2XgIQxqgA-W"
      },
      "source": [
        "feature_maps = vgg_vis.predict( content_image )\n",
        "show_layer_representations( feature_maps=feature_maps, layer_names=VISUALIZED_LAYERS )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SZMOFihiuNb"
      },
      "source": [
        "show_layer_representations( feature_maps=feature_maps, layer_names=VISUALIZED_LAYERS )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naK3BiNX3VRC"
      },
      "source": [
        "<a name=\"3\"></a>\n",
        "##3 Little helpers: Cost function\n",
        "Define the cost functions given in the original paper, the computations are done in seveal steps:\n",
        "0.   Define style layers and their weight \n",
        "1.   Compute cost due to changes of the content\n",
        "2.   Compute cost due the differences in the Gram marices of a given layer\n",
        "3.   Compute Gram matrix\n",
        "4.   Compute cost due to different styles\n",
        "5.   Compute total cost\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIHXdBG-3VRD"
      },
      "source": [
        "# choose style layers and their corresponding weights\n",
        "STYLE_LAYERS = [\n",
        "    ('block1_conv1', 0.2),\n",
        "    ('block2_conv1', 0.2),\n",
        "    ('block3_conv1', 0.2),\n",
        "    ('block4_conv1', 0.2),\n",
        "    ('block5_conv1', 0.2)]\n",
        "\n",
        "@tf.function()\n",
        "def compute_content_cost( content_output, generated_output ):\n",
        "  \"\"\" little helper function\n",
        "  Computes the content cost\n",
        "    \n",
        "  Arguments:\n",
        "    a_C --  intermediate representation of content of image C\n",
        "            tensor of shape (1, n_H, n_W, n_C)\n",
        "    a_G --  intermediate representation of content of image C\n",
        "            tensor of shape (1, n_H, n_W, n_C)\n",
        "    \n",
        "  Returns: \n",
        "    J_content -- scalar, loss due to content\n",
        "  \"\"\"\n",
        "  a_C = content_output[-1]\n",
        "  a_G = generated_output[-1]\n",
        "    \n",
        "  # dimensions from a_G\n",
        "  m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
        "    \n",
        "  # reshape a_C and a_G\n",
        "  a_C_unrolled = tf.transpose( tf.reshape( a_C, shape=[m, -1, n_C] ), perm=[0,2,1] )\n",
        "  a_G_unrolled = tf.transpose( tf.reshape( a_G, shape=[m, -1, n_C] ), perm=[0,2,1] )\n",
        "    \n",
        "  # compute the cost with tensorflow (≈1 line)\n",
        "  J_content =  tf.reduce_sum( tf.square( tf.subtract( a_C_unrolled, a_G_unrolled ) ) ) / ( 4 * n_H*n_W*n_C )\n",
        "    \n",
        "  return J_content\n",
        "\n",
        "def compute_layer_style_cost(a_S, a_G):\n",
        "  \"\"\" little helper function\n",
        "  Compute style cost of a given layer\n",
        "\n",
        "  Arguments:\n",
        "    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S \n",
        "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n",
        "    \n",
        "  Returns: \n",
        "    J_style_layer -- tensor representing a scalar value, style cost\n",
        "  \"\"\"\n",
        "  m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
        "  #print( a_G.get_shape().as_list() )\n",
        "    \n",
        "  # reshape images into shape ( n_C, n_H * n_W )\n",
        "  a_S = tf.squeeze( tf.transpose( tf.reshape( a_S, shape=[-1, n_H*n_W, n_C] ), perm=[2,1,0] ), axis=2 )\n",
        "  a_G = tf.squeeze( tf.transpose( tf.reshape( a_G, shape=[-1, n_H*n_W, n_C] ), perm=[2,1,0] ), axis=2 )\n",
        "    \n",
        "  # computing gram_matrices\n",
        "  GS = gram_matrix( a_S )\n",
        "  GG = gram_matrix( a_G )\n",
        "    \n",
        "  # computeing the loss\n",
        "  J_style_layer = tf.reduce_sum( tf.square( tf.subtract( GS, GG ) ) ) / ( 2 * n_H*n_W*n_C )**2\n",
        "    \n",
        "  return J_style_layer\n",
        "\n",
        "def gram_matrix(A):\n",
        "  \"\"\" little helper's helper function\n",
        "    \n",
        "  Arguemts:\n",
        "    A -- matrix of shape (n_C, n_H*n_W)\n",
        "    \n",
        "  Returns:\n",
        "    GA -- Gram matrix of A, of shape (n_C, n_C)\n",
        "  \"\"\"  \n",
        "  GA = tf.matmul( A, tf.transpose( A ) )\n",
        "\n",
        "  return GA\n",
        "\n",
        "@tf.function\n",
        "def compute_style_cost( style_image_output, generated_image_output, style_layers=STYLE_LAYERS ):\n",
        "  \"\"\" little helper function\n",
        "  Compute the overall style cost from several chosen layers\n",
        "    \n",
        "  Arguemnts:\n",
        "    style_image_output -- our tensorflow model's hidden layers representations\n",
        "    generated_image_output -- our tensorflow model's hidden layers representations\n",
        "    style_layers -- A python list containing:\n",
        "                        - the names of the layers we would like to extract style from\n",
        "                        - a coefficient for each of them\n",
        "    \n",
        "  Returns: \n",
        "    J_style -- tensor representing a scalar value, style cost\n",
        "\n",
        "  Additional remark:\n",
        "    the observed layers are the style layers and the content layer (whose feature map is removed first)\n",
        "  \"\"\"\n",
        "  # initialize\n",
        "  J_style = 0\n",
        "\n",
        "  # hidden layer activation from selected layer (last element contains the not to be used image)\n",
        "  a_S = style_image_output[ :-1 ]\n",
        "\n",
        "  # hidden layers activation (same here)\n",
        "  a_G = generated_image_output[ :-1 ]\n",
        "\n",
        "  # collect single layer's costs and sum up\n",
        "  for i, weight in zip( range( len(a_S) ), style_layers ):  \n",
        "      J_style_layer = compute_layer_style_cost( a_S[i], a_G[i] )\n",
        "      J_style += weight[1] * J_style_layer\n",
        "\n",
        "  return J_style\n",
        "\n",
        "@tf.function\n",
        "def total_cost( J_content, J_style, alpha = 10, beta = 40 ):\n",
        "  \"\"\" little helper funtion\n",
        "  Compute the total cost function\n",
        "    \n",
        "  Arguments:\n",
        "    J_content -- content cost coded above\n",
        "    J_style -- style cost coded above\n",
        "    alpha -- hyperparameter weighting the importance of the content cost\n",
        "    beta -- hyperparameter weighting the importance of the style cost\n",
        "    \n",
        "  Returns:\n",
        "    J -- total cost as defined by the formula above.\n",
        "  \"\"\"\n",
        "  J = alpha * J_content + beta * J_style\n",
        "  return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6en95If5VUSv"
      },
      "source": [
        "<a name=\"4\"></a>\n",
        "##4 Optimisation problem\n",
        "First, helper functions to clip and convert images are defined, then the model is set up and images preprocessed. Then a training step is defined as a function and finally the iterations are perfomed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFHMSxEBddoP"
      },
      "source": [
        "def clip_0_1( image ):\n",
        "  \"\"\"\n",
        "  truncate pixels in the tensor to be between 0 and 1\n",
        "    \n",
        "  Arguments:\n",
        "    image -- tensor\n",
        "    \n",
        "  Returns:\n",
        "    tensor\n",
        "  \"\"\"\n",
        "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "def tensor_to_image( tensor, keepdims=False ):\n",
        "  \"\"\"\n",
        "  converts the given tensor into a PIL image\n",
        "    \n",
        "  Arguments:\n",
        "    tensor -- tensor\n",
        "    keepdims -- boolean\n",
        "    \n",
        "  Returns:\n",
        "    Image -- PIL image\n",
        "  \"\"\"\n",
        "  tensor = tensor * 255\n",
        "  tensor = np.array(tensor, dtype=np.uint8)\n",
        "  if np.ndim(tensor) > 3:\n",
        "      assert tensor.shape[0] == 1\n",
        "      if not keepdims:\n",
        "        tensor = tensor[0]\n",
        "  return Image.fromarray(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlPs0eznvbMF"
      },
      "source": [
        "The content representation is taken from the last convolutional layer and a helper model is created with the style layers previously defined and the content layer as outputs of the helper model. For computations, images are converted into float32 tensor objects. The initial generated image is simply a noisy content image.\n",
        "\n",
        "Compare content loss to style loss. Since the proprocessed content image is a noisy version of the original content image, its loss is quite small, where the style loss is quite large. The objectiv of interest is the sum of these two losses. During optimisation, the style loss will decrease at cost of increasing the content loss. But anyways, that is desired."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qy_Mq2N3VRF"
      },
      "source": [
        "# build model to retrieve the activations of the style layers and the content layer\n",
        "CONTENT_LAYER = [('block5_conv4', 1)]\n",
        "layer_outputs = get_layer_outputs( vgg, STYLE_LAYERS + CONTENT_LAYER )\n",
        "pp.pprint( layer_outputs )\n",
        "vgg_model_outputs = tf.keras.Model( vgg.input, layer_outputs )\n",
        "print( vgg_model_outputs )\n",
        "\n",
        "# preprocess content image and retrieve hidden layer's responses\n",
        "print( type( content_image ) ) \n",
        "preprocessed_content =  tf.Variable(tf.image.convert_image_dtype( content_image, tf.float32 ) )\n",
        "a_C = vgg_model_outputs( preprocessed_content )\n",
        "print(type(a_C))\n",
        "\n",
        "# add noise to content image and retrieve hidden layer's respones\n",
        "noise = tf.random.uniform( tf.shape( preprocessed_content ), 0, 0.5 )\n",
        "generated_image = tf.add( preprocessed_content, noise )\n",
        "generated_image = tf.Variable( clip_0_1( generated_image ) )\n",
        "print(type(generated_image))\n",
        "a_G = vgg_model_outputs( generated_image )\n",
        "\n",
        "# compute the content cost\n",
        "J_content = compute_content_cost( a_C, a_G )\n",
        "print(J_content)\n",
        "\n",
        "# preprocess style image and retrieve hidden layer's responses\n",
        "preprocessed_style =  tf.Variable( tf.image.convert_image_dtype( style_image, tf.float32 ) )\n",
        "a_S = vgg_model_outputs( preprocessed_style )\n",
        "\n",
        "# Compute the style cost\n",
        "J_style = compute_style_cost( a_S, a_G, STYLE_LAYERS )\n",
        "print(J_style)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE-SBCBawU8u"
      },
      "source": [
        "Here, the training step funtion is defined. Note that the generated image needs to be a tf.Variable. Not the following: The tf.function decoration lets the function be executed in graph mode allowing a much faster run. On the first run, a computation graph is built by running Python commands only, and they are translated into Tensorflow objects. On each following run, only Tensorflow commands are executed, but not Python ones. Whenever a non-tensorflow variable is changed after the first run, that yields an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxDtf6LNdmOm"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
        "\n",
        "@tf.function()\n",
        "def train_step( generated_image, a_S, a_C ):\n",
        "  \"\"\"\n",
        "  train step for generating the style transferred image\n",
        "\n",
        "  Arguments:\n",
        "    generated_image -- tensor, the image to be optimsed\n",
        "    a_S -- list of tensors, model outputs of the style image\n",
        "    a_C -- list of tensors, model outputs of the content image\n",
        "  \n",
        "  Returns:\n",
        "    J -- loss\n",
        "  \"\"\"\n",
        "  # let tf compute the gradient along computations for loss\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch( generated_image )\n",
        "\n",
        "    # get hidden layer's responses of generated image\n",
        "    a_G = vgg_model_outputs( generated_image )\n",
        "\n",
        "    # style cost\n",
        "    J_style = compute_style_cost( a_S[:-1], a_G[:-1] )\n",
        "    # content cost\n",
        "    J_content = compute_content_cost( a_C, a_G )\n",
        "    # total cost\n",
        "    J = total_cost( J_content, J_style )\n",
        "\n",
        "  grad = tape.gradient( J, generated_image )\n",
        "\n",
        "  optimizer.apply_gradients( [ ( grad, generated_image ) ] )\n",
        "  generated_image.assign( clip_0_1( generated_image ) )\n",
        "  #print(type(generated_image))\n",
        "  \n",
        "  return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoFYNk4Cw4a9"
      },
      "source": [
        "A single train step. For test purposes only. Note that for that curious reason just mentioned, executing the following cell must be followed by executing the train step cell before executing the optimisation loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRaO5xDpeIQM"
      },
      "source": [
        "train_step( generated_image, a_S, a_C )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9WhO7i6xUHm"
      },
      "source": [
        "Optimisation loop. Ensure to execute the train step definition right before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LalI2Cttuiv2"
      },
      "source": [
        "def get_train_loop():\n",
        "  #@tf.function()\n",
        "  def train_loop( generated_image, a_S, a_C, epochs=1, intermediate=7, save_image=False ):\n",
        "    \"\"\"\n",
        "    training loop for generating the style transferred image\n",
        "\n",
        "    Arguments:\n",
        "      generated_image -- tensor, the image to be optimsed\n",
        "      a_S -- list of tensors, model outputs of the style image\n",
        "      a_C -- list of tensors, model outputs of the content image\n",
        "      epochs -- numbwe of epochs to run the optimisation procedure\n",
        "      intermediate -- show intermediate resulte every ... steps\n",
        "      save_image -- save intermediate images to disk\n",
        "  \n",
        "    Returns:\n",
        "      J -- loss\n",
        "    \"\"\"\n",
        "    intermediate_images = np.zeros( (3, IMG_HEIGHT, IMG_WIDTH, 3), dtype='uint8' )\n",
        "    intermediate_titles = ['0','0','0']\n",
        "    pos = 0\n",
        "\n",
        "    for i in range( epochs ):\n",
        "      train_step( generated_image, a_S, a_C )\n",
        "      if i % intermediate == 0:\n",
        "          if save_image:\n",
        "            image.save(f\"{content_image_name[:-4]}_{i}.jpg\")\n",
        "          intermediate_images[pos,:,:,:] = tensor_to_image( generated_image )\n",
        "          intermediate_titles[pos] = str(i)\n",
        "          pos += 1\n",
        "          if pos==3:\n",
        "            pos = 0\n",
        "            plot_row( intermediate_images, intermediate_titles, 3 )\n",
        "    if pos in [1, 2]:\n",
        "      plot_row( intermediate_images, intermediate_titles, num=pos )\n",
        "    print( 'ferddich' )\n",
        "  return train_loop  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrHnO6vArkiW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRhWmdAKdzuw"
      },
      "source": [
        "# Show the generated image at some epochs\n",
        "# Uncoment to reset the style transfer process. You will need to compile the train_step function again \n",
        "EPOCHS = 59\n",
        "\n",
        "train = get_train_loop()\n",
        "train( generated_image, a_S, a_C, epochs=EPOCHS, intermediate=7 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_UGyEGmxfOR"
      },
      "source": [
        "Plot the result next to the style image and the original content image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s2o6Eb_h6xR"
      },
      "source": [
        "# Show the 3 images in a row\n",
        "plot_row( np.concatenate([ content_image, style_image, \n",
        "                          tf.cast( generated_image*255, dtype='uint8') ] , axis=0), \n",
        "         [ 'Content_image', 'Style_image', 'Generated image' ], 3 )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeSibuO0kRU4"
      },
      "source": [
        "<a name=\"5\"></a>\n",
        "##5 Reconstruction of the style\n",
        "Here the optimisation problem is tackled for the style loss only. More precisely for any given number of the chosen style layers starting from the first one (similar to the original paper: at first the first chosen layer, then the first two chosen layers and so on). The image on which the optimisation is run is initialised with pure noise, and once the given number of epochs is done, the result is handed over to be optimised with one additional layer. \n",
        "\n",
        "The images show how structures emerge and in which way the layers contribute. At first, noise stays noise, but changes its colour. During the second run with two layers, grains emerge from the noise giving the image som granularity. After the third run the characteristic usage of the brush gets visible. Finally, abstract sructures appear in the style of the style image, but without showing any shape of any object.\n",
        "\n",
        "At first the loss function, which is almost similar to the compute_style cost function apart from extended functionality (could be coded more elegantly), is defined.\n",
        "\n",
        "Up to now, the code runs in eager mode only. Any of my attempts to get it run in graph mode did not succeed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX2azlxzko6K"
      },
      "source": [
        "def get_compute_style_loss():\n",
        "  #@tf.function\n",
        "  def compute_style_loss( a_S, a_G, layers_with_weights ):\n",
        "    \"\"\" little helper function\n",
        "    compute the style loss of given number of style layers\n",
        "\n",
        "    Arguments:\n",
        "      a_S -- response of vgg on the style image\n",
        "      a_G -- response of vgg on the generated image\n",
        "      layers_with_weights -- python list containing:\n",
        "                            - the names of the layers the style is extracted from\n",
        "                            - a coefficient for each of them\n",
        "                          ! \n",
        "      n_L -- number of layers to be considered starting from the beginning of the model\n",
        "\n",
        "    Returns:\n",
        "      J_style -- loss of the style corresponding to the number of layers n_L\n",
        "    \"\"\"\n",
        "    # initialize\n",
        "    J_style = np.zeros( (1,), dtype='float32' )\n",
        "    \n",
        "    # collect single layer's costs and sum up\n",
        "    for i, weight in enumerate( layers_with_weights ):  \n",
        "      J_style_layer = compute_layer_style_cost( a_S[i], a_G[i] )\n",
        "      J_style += weight[1] * J_style_layer\n",
        "\n",
        "    return J_style\n",
        "\n",
        "  return compute_style_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ah_C-JHCnb_"
      },
      "source": [
        "Next, the train step function and the train loop function are created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a47jSnEEdcIf"
      },
      "source": [
        "def get_style_train_step():  \n",
        "  #@tf.function\n",
        "  def style_train_step( generated_image, a_S, style_layers, loss ):\n",
        "    \"\"\"\n",
        "    single optimizatiion step for the style extraction\n",
        "\n",
        "    Arguments:\n",
        "    generated_image -- tensor, image that is optimized\n",
        "    a_S -- list of tensors, model's response to style image\n",
        "    style_layers -- python list containing:\n",
        "                          - the names of the layers the style is extracted from\n",
        "                          - a coefficient for each of them\n",
        "    loss -- function that computes the loss\n",
        "\n",
        "    Returns:\n",
        "    J -- computed loss\n",
        "    \"\"\"\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      # retrieve activations\n",
        "      a_G = vgg_model_outputs( generated_image )\n",
        "\n",
        "      # compute loss\n",
        "      J = loss( a_S, a_G, style_layers )\n",
        "\n",
        "    grad = tape.gradient( J, generated_image )\n",
        "\n",
        "    # perform descent step and clip to unit intervall\n",
        "    optimizer.apply_gradients( [ ( grad, generated_image ) ] )\n",
        "    generated_image.assign( clip_0_1( generated_image ) )\n",
        "  \n",
        "    return J\n",
        "  return style_train_step\n",
        "\n",
        "def get_style_train_loop():\n",
        "  def style_train_loop( generated_image, a_S, style_layers, epochs=1, intermediate=7, save_image=False ):\n",
        "    \"\"\"\n",
        "    optimizatiion loop for the style extraction\n",
        "\n",
        "    Arguments:\n",
        "      generated_image -- tensor, image that is optimized\n",
        "      a_S -- list of tensors, model's response to style image\n",
        "      style_layers -- python list containing:\n",
        "                            - the names of the layers the style is extracted from\n",
        "                            - a coefficient for each of them\n",
        "      epochs -- number of epochs to run\n",
        "      intermediate -- after this number of steps the current generated image is shown\n",
        "      save_image -- in case of these images shall be saved to disk\n",
        "    \"\"\"\n",
        "\n",
        "    # initialise the variables that hold the intermediate images and their titles\n",
        "    intermediate_images = np.zeros( (3, IMG_HEIGHT, IMG_WIDTH, 3), dtype='uint8' )\n",
        "    intermediate_titles = ['0','0','0']\n",
        "    pos = 0\n",
        "    J = np.zeros( (epochs,) )\n",
        "    train = get_style_train_step()\n",
        "    compute_style_loss = get_compute_style_loss()\n",
        "\n",
        "    for i in range( epochs ):\n",
        "      # perform a training step\n",
        "      J[i] = train( generated_image, a_S, style_layers, compute_style_loss )\n",
        "      # image handling\n",
        "      if i % intermediate == 0:\n",
        "          if save_image:\n",
        "            image.save(f\"{style_image_name[:-4]}_extractedlayers{len(style_layers)}_{i}.jpg\")\n",
        "          intermediate_images[pos,:,:,:] = tensor_to_image( generated_image )\n",
        "          intermediate_titles[pos] = str(i)\n",
        "          pos += 1\n",
        "          if pos==3:\n",
        "            pos = 0\n",
        "            plot_row( intermediate_images, intermediate_titles, 3 )\n",
        "    if pos in [1, 2]:\n",
        "      plot_row( intermediate_images, intermediate_titles, num=pos, J=J )\n",
        "    else:\n",
        "      plot_row( intermediate_images, intermediate_titles, num=0, J=J )\n",
        "    #print( f'loss in each step {J}' )\n",
        "    print( 'ferddich' )\n",
        "  return style_train_loop  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6LdzlfmNKVP"
      },
      "source": [
        "Finally, the final loop that executes the runs through the training loop follows. Note that with 5 style layers more epochs are needed than with one or two style layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVS3DTLYgEjm"
      },
      "source": [
        "# Show the generated image at some epochs\n",
        "# Uncoment to reset the style transfer process. You will need to compile the train_step function again \n",
        "EPOCHS = 43\n",
        "INTERMEDIATE = 7\n",
        "\n",
        "# prepare initial image to be purely noisy\n",
        "noise = tf.random.uniform( tf.shape( preprocessed_style ), 0, 0.5 )\n",
        "generated_image = tf.Variable( clip_0_1( noise ), trainable=True, dtype=tf.float32 )\n",
        "\n",
        "# loop: optimise for the first n_L layers\n",
        "for n_L in range( len( STYLE_LAYERS ) ):\n",
        "\n",
        "  print( f\"Optimize against {n_L+1} layer(s)\" )\n",
        "  style_train = get_style_train_loop()\n",
        "  style_train( generated_image, a_S, STYLE_LAYERS[ :n_L+1 ], epochs=EPOCHS, intermediate=INTERMEDIATE )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7Tbr3wIKE4G"
      },
      "source": [
        "That's it, the following blocks are just there and do not intend to be run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg_0Iot8kXps"
      },
      "source": [
        "class StyleTrain():\n",
        "  def __init__( self, model_outputs, a_S, style_layers, n_L, cmp_layer_style_cost ):\n",
        "    self._n_L = n_L\n",
        "    self._optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
        "    self._model_outputs = model_outputs\n",
        "    self._a_S = a_S\n",
        "    self._style_layers = style_layers\n",
        "    #self.J_style = 0\n",
        "    self.cmp_layer_style_cost = cmp_layer_style_cost\n",
        "\n",
        "  #training step method\n",
        "  @tf.function\n",
        "  def __call__(self, generated_image ): #style_train_step( generated_image, n_L=1 ):\n",
        "    print( \"Inputs to train step: \", type( generated_image ), generated_image )\n",
        "    #print(\"trainables \", tf.trainable_variables )\n",
        "    with tf.GradientTape( ) as tape: #watch_accessed_variables=False ) as tape:\n",
        "      tape.watch( generated_image )\n",
        "      #tf.print( type( generated_image ) )\n",
        "      # retrieve activations\n",
        "      a_G = self._model_outputs( generated_image )\n",
        "    \n",
        "      # compute loss\n",
        "      J = self.compute_style_loss( self._a_S, a_G )\n",
        "\n",
        "    grad = tape.gradient( J, generated_image )\n",
        "    tf.print( \"nach GradTape: \", type( grad ) )\n",
        "    self._optimizer.apply_gradients( [ ( grad, generated_image ) ] )\n",
        "    #generated_image.assign( clip_0_1( generated_image ) )\n",
        "  \n",
        "    return J\n",
        "\n",
        "  @tf.function\n",
        "  def compute_style_loss( self, generated_image_output, STYLE_LAYERS=STYLE_LAYERS ):\n",
        "    \"\"\"\n",
        "    compute the style loss of given number of style layers\n",
        "\n",
        "    Inputs:\n",
        "    style_layer_output -- response of vgg on the style image\n",
        "    generated_layer_output -- response of vgg on the generated image\n",
        "    STYLE_LAYERS -- python list containing:\n",
        "                        - the names of the layers the style is extracted from\n",
        "                        - a coefficient for each of them\n",
        "\n",
        "    Output:\n",
        "    J_style -- loss of the style corresponding to the number of layers n_L\n",
        "    \"\"\"\n",
        "    # initialize\n",
        "    J_style = 0\n",
        "    #num = tf.gather( n_L, 0 ).numpy()\n",
        "    #print(\"CSL lengths: \",type(style_image_output),len(style_image_output),len(generated_image_output),len(STYLE_LAYERS),n_L)\n",
        "\n",
        "    # extract style responses of n_L layers \n",
        "    #layers = STYLE_LAYERS[ :n_L ]\n",
        "\n",
        "    # collect single layer's costs and sum up\n",
        "    for i in range( self._n_L ):  \n",
        "      J_style_layer = self.cmp_layer_style_cost( a_S[i], a_G[i] )\n",
        "      J_style += self._style_layers[i][1] * J_style_layer\n",
        "\n",
        "    return J_style"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A56S9rgp8LLh"
      },
      "source": [
        "print( tf.autograph.to_code(style_train_step.python_function))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nwlAl_x1IEd"
      },
      "source": [
        "Training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJxKkf7dZ70N"
      },
      "source": [
        "# Show the generated image at some epochs\n",
        "# Uncoment to reset the style transfer process. You will need to compile the train_step function again \n",
        "EPOCHS = 13\n",
        "pos = 1\n",
        "fig = plt.figure(figsize=(16, 4))\n",
        "J = np.zeros( (EPOCHS,) )\n",
        "\n",
        "noise = tf.random.uniform( tf.shape( preprocessed_style ), 0, 0.5 )\n",
        "generated_image = tf.Variable( clip_0_1( noise ), trainable=True )\n",
        "\n",
        "print(type(generated_image))\n",
        "print( generated_image.get_shape().as_list())\n",
        "print(type(generated_image[0]))\n",
        "\n",
        "for n_L in range( 2 ): #len( STYLE_LAYERS ) ):\n",
        "  #n = tf.Variable( n_L )\n",
        "  print( f\"Optimize against {n_L+1} layer(s)\" )\n",
        "  style_train_step = StyleTrain( vgg_model_outputs, a_S, STYLE_LAYERS, n_L, compute_layer_style_cost )\n",
        "  for i in range( EPOCHS ):\n",
        "    J[i] = style_train_step( generated_image )\n",
        "    if i % 3 == 0:\n",
        "        ax = fig.add_subplot( 1, 3, pos )\n",
        "        image = tensor_to_image( generated_image )\n",
        "        imshow( image )\n",
        "        ax.title.set_text( f\"{i}\" )\n",
        "        #image.save(f\"{content_image_name[:-4]}_{i}.jpg\")\n",
        "        if pos==1:\n",
        "          pos = 2\n",
        "        elif pos==2:\n",
        "          pos = 3\n",
        "        else:\n",
        "          pos = 1\n",
        "          plt.show() \n",
        "          fig = plt.figure(figsize=(16, 4))\n",
        "\n",
        "  ax = fig.add_subplot( 1, 3, pos )\n",
        "  plt.plot( J )\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}